---
title: "Human Activity Recognition"
author: "Tianxiang(Ivan) Liu"
date: "18 September 2014"
output: html_document
---

#### Executive Summary

Human Activity Recognition - HAR - has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research community, especially for the development of context-aware systems. There are many potential applications for HAR, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.

The objective of this report is to establish a classfier to predict different classe types based on data collected from mobile devices.

```{r, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
setwd("C:\\Users\\Ivan.Liuyanfeng\\Desktop\\Data_Mining_Work_Space\\Human-Activity-Recognition")
library(caret)
```

---

#### Data Exploration and Cleaning
First of all, we need to do some data exploration works and clean the datasets. 
Here we detect the null values of each variable and find there are a bunch of columns have over 95% NA values which is too big to make the imputation. So we decide to delete those columns directly.
Then we also test the non zero vairable and delete the Id and usernames columns which we think are meaningless to the model.
At last we get a clean training set which contains 19622 observations and 57 variables for modeling task.

```{r, echo=TRUE,warning=FALSE,message=FALSE,error=FALSE}
# load data
train <- read.table('data/pml-training.csv', stringsAsFactor=F, sep=','
                    ,header = T,na.strings = c("NA",""))
# detect the NA percent of each column
na_train <- sapply(1:length(names(train)), function(i, na_base=c()){
    na_col <- mean(is.na(train[i]))
    na_base <- c(na_base, na_col)})
names(na_train) <- names(train)
# eliminate the column with over 50% NA value
na_index <- na_train < .5
train2 <- train[,na_index]
train2 <- train2[,-c(1, 2)]
# non zero variable
nzv <- nearZeroVar(train2,saveMetrics = F)
training <- train2[,-nzv]
dim(training)
```

---

#### Modeling
Then, we choose gbm with cross validation to do the modeling work. So we can avoid the overfitting problem to some extent. 
We also split the raw datasets into training and testing parts so we can measure the out of sample error later.
After modeling, we first apply our model to training set and we can see the confusion matrix has revealed that the model can perfectly classify all five classes based on training dataset. The sensitivity and specificity for all classes are 100%. For now, the classifer looks prety good. 
Following plot also indicates the top 10 important variables to predict different classes of users.

```{r, echo=TRUE,warning=FALSE,message=FALSE,error=FALSE,fig.height=6, fig.width=8}
set.seed(888)
# split train and test
index <- createDataPartition(training$classe, p=0.8, list=F)
train_md <- training[index,]
test_md <- training[-index,]
# modeling
fitControl <- trainControl(method = "cv",number = 10)
gbmGrid <-  expand.grid(interaction.depth = 5, n.trees = 300, shrinkage = 0.1)
fit<- train(as.factor(classe)~., data=train_md, method = 'gbm', trControl=fitControl, tuneGrid = gbmGrid)
gbmImp <- varImp(fit,scale=F)
plot(gbmImp, top=10)
```

---

#### Out of Sample Error
```{r, echo=TRUE,warning=FALSE,message=FALSE,error=FALSE}
pred <- predict(fit, train_md)
result <- confusionMatrix(pred, train_md$classe)
pred_t <- predict(fit, test_md)
result_t <- confusionMatrix(pred_t, test_md$classe)
```

---

#### Prediction
After test the model on training data. Then we will apply the classifier to test datasets.
First, we load the test dataset and do the exactly same preprocess works as we've done on training dataset. Then we apply the classifier on it and get the results as following. 

```{r, echo=TRUE,warning=FALSE,message=FALSE,error=FALSE}
test <- read.table('data/pml-testing.csv', stringsAsFactor=F, sep=','
                    ,header = T,na.strings = c("NA",""))
# test2 <- test[,na_index]
# testing <- test2[,-nzv]
pred_test <- predict(fit, newdata=test)
results <- data.frame(test$problem_id, test$user_name, pred_test)
names(results) <- c('Problem_id','User_name','Classe')
results
```

---

#### Error Appropriately with Cross Validation
```{r, echo=TRUE,warning=FALSE,message=FALSE,error=FALSE}

```

---

#### Conclusion

---

Thank you for reading!
Tianxiang Liu